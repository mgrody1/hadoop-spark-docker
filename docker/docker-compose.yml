version: '3.8'

services:
  namenode:
    image: apache/hadoop:3.3.5
    container_name: namenode
    hostname: namenode
    user: root
    environment:
      - CLUSTER_NAME=survivor-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - namenode:/opt/hadoop/data/namenode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./datasets:/datasets  # Mount survivor dataset here
    ports:
      - "9870:9870"  # HDFS UI
      - "8020:8020"  # HDFS RPC
    networks:
      hadoop-net:
        ipv4_address: 172.20.0.2
    command: [ "/bin/bash", "/start-hdfs.sh" ]

  datanode:
    image: apache/hadoop:3.3.5
    container_name: datanode
    hostname: datanode
    user: root
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - datanode:/opt/hadoop/data/datanode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
    depends_on:
      - namenode
    networks:
      hadoop-net:
        ipv4_address: 172.20.0.3
    command: [ "/bin/bash", "/init-datanode.sh" ]

  resourcemanager:
    image: apache/hadoop:3.3.5
    container_name: resourcemanager
    hostname: resourcemanager
    user: root
    environment:
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
    ports:
      - "8088:8088"  # YARN UI
    networks:
      hadoop-net:
        ipv4_address: 172.20.0.4
    command: [ "/bin/bash", "-c", "/opt/hadoop/bin/yarn resourcemanager" ]

  nodemanager:
    image: apache/hadoop:3.3.5
    container_name: nodemanager
    hostname: nodemanager
    user: root
    environment:
      - YARN_CONF_yarn_nodemanager_aux_services=mapreduce_shuffle
    depends_on:
      - resourcemanager
    networks:
      hadoop-net:
        ipv4_address: 172.20.0.5
    command: [ "/bin/bash", "-c", "/opt/hadoop/bin/yarn nodemanager" ]

  spark-master:
    image: bde2020/spark-master:3.0.1-hadoop3.2
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "8081:8080"  # Spark UI
      - "7077:7077"  # Spark Master
    networks:
      hadoop-net:
        ipv4_address: 172.20.0.6

  spark-worker:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      hadoop-net:
        ipv4_address: 172.20.0.7

volumes:
  namenode:
  datanode:

networks:
  hadoop-net:
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
