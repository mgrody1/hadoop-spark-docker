version: '3.8'

services:
  hadoop-namenode:
    image: apache/hadoop:3.3.5
    container_name: hadoop-namenode
    hostname: hadoop-namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./hadoop_namenode:/opt/hadoop/data/nameNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop  # Ensure config persists
      - ./start-hdfs.sh:/start-hdfs.sh
    ports:
      - "9870:9870"  # Web UI
      - "8020:8020"  # HDFS NameNode RPC port
    command: [ "/bin/bash", "/start-hdfs.sh" ]
    networks:
      hadoop-net:
        ipv4_address: 172.21.0.2

  hadoop-datanode:
    image: apache/hadoop:3.3.5
    container_name: hadoop-datanode
    hostname: hadoop-datanode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./hadoop_datanode:/opt/hadoop/data/dataNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop  # Ensure config persists
      - ./init-datanode.sh:/init-datanode.sh
    depends_on:
      - hadoop-namenode
    command: [ "/bin/bash", "/init-datanode.sh" ]
    networks:
      hadoop-net:
        ipv4_address: 172.21.0.3

  spark-master:
    image: bde2020/spark-master:3.0.1-hadoop3.2
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "8081:8080"  # Spark Web UI
      - "7077:7077"  # Spark Master
    networks:
      - hadoop-net

  spark-worker:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - hadoop-net

  cassandra:
    image: cassandra:latest
    container_name: cassandra
    ports:
      - "9042:9042"  # Cassandra CQL Port
    networks:
      - hadoop-net
    volumes:
      - cassandra_data:/var/lib/cassandra

networks:
  hadoop-net:
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16

volumes:
  cassandra_data:
